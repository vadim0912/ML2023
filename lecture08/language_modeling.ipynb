{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3645291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, Iterable, List, Tuple\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywidgets import interact\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dfbf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_PATH = \"./data/lm_dataset_jsonl.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be599d35",
   "metadata": {},
   "source": [
    "Uncomment if you are using colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996d0beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir ./data\n",
    "# !wget https://raw.githubusercontent.com/vadim0912/ML2023/main/lecture08/data/lm_dataset_jsonl.gz -O $DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad748d45",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb92aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(DATA_PATH, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e678217",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_by_year = df[\"timestamp\"].apply(lambda x: x.year).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e674293",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(text_by_year.index, text_by_year.values)\n",
    "plt.xlabel(\"year\")\n",
    "plt.ylabel(\"number of pages\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79859758",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df[\"text\"].apply(lambda x: x.split(\"\\n\")).explode().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0b01b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_by_length = texts.str.len().value_counts().sort_index()\n",
    "\n",
    "plt.plot(text_by_length.index, text_by_length.values)\n",
    "plt.xlabel(\"length\")\n",
    "plt.ylabel(\"number of pages\")\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeb30e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_tokenize(text: str) -> List[str]:\n",
    "    text = text.lower().replace(\"ё\", \"е\")\n",
    "    text = re.sub('[^а-я0-9a-z,.\\-?!–«»\"\": ]', \" \", text)\n",
    "    text = re.sub(\" +\", \" \", text).strip()\n",
    "    text = nltk.wordpunct_tokenize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640de8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = texts.apply(normalize_and_tokenize).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c663b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = train_test_split(\n",
    "    tokenized_texts, test_size=0.05, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd49339",
   "metadata": {},
   "source": [
    "# N-gram Language Model\n",
    "\n",
    "$$\n",
    "    P(w_1, \\dots, w_k) = \\prod_t P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1}).\n",
    "$$\n",
    "\n",
    "$$\n",
    "    P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1}) \\approx \\frac{\\text{count} (w_{t - n + 1} \\dots, w_{t - 1}, w_t)}{\\displaystyle \\sum_w \\text{count} (w_{t - n + 1} \\dots w_{t - 1}, w)}=\\frac{\\text{count} (w_{t - n + 1} \\dots, w_{t - 1}, w_t)}{\\text{count} (w_{t - n + 1} \\dots w_{t - 1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d05e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = \"<BOS>\"\n",
    "EOS = \"<EOS>\"\n",
    "\n",
    "ngrams_config = {\n",
    "    \"pad_left\": True,\n",
    "    \"pad_right\": True,\n",
    "    \"left_pad_symbol\": BOS,\n",
    "    \"right_pad_symbol\": EOS,\n",
    "}\n",
    "\n",
    "\n",
    "def build_ngram_counts(\n",
    "    tokenized_texts: Iterable[Iterable[str]], n: int\n",
    ") -> Dict[Tuple[str, ...], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    встречаемость слова при условии (n - 1) предыдущего слова\n",
    "\n",
    "    для n = 2:\n",
    "        {\n",
    "            ('добрый',): {'день': 273, 'вечер': 55, 'путь': 8, ...},\n",
    "            ('машинное',): {'масло': 2, 'отделение': 6, 'обучение': 4, ...}\n",
    "            ...\n",
    "        }\n",
    "\n",
    "    для n = 3:\n",
    "         {\n",
    "            ('<BOS>', '<BOS>'): {'мэр': 22, 'выпуск': 40, ...},\n",
    "            ('<BOS>', 'мэр'): {'москвы': 3, 'перми': 3, ...},\n",
    "            ...\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    counts = defaultdict(Counter)\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb48054",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    def __init__(self, tokenized_texts: Iterable[Iterable[str]], n: int) -> None:\n",
    "        self.n: int = n\n",
    "        self.probs: Dict[Tuple[str, ...], Dict[str, float]] = defaultdict(Counter)\n",
    "\n",
    "    def get_token_distribution(self, prefix: List[str]) -> Dict[str, float]:\n",
    "        prefix = prefix[max(0, len(prefix) - self.n + 1) :]\n",
    "        prefix = [BOS] * (self.n - 1 - len(prefix)) + prefix\n",
    "        return self.probs[tuple(prefix)]\n",
    "\n",
    "    def get_next_token_prob(self, prefix: List[str], token: str) -> float:\n",
    "        return self.get_token_distribution(prefix)[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17975f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = LanguageModel(train_set, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7141457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(\n",
    "    distribution: Dict[str, float],\n",
    "    top_k: int = None,\n",
    "    title: str = None,\n",
    "    xlim: bool = True,\n",
    ") -> None:\n",
    "    data = sorted(distribution.items(), key=lambda x: -x[1])\n",
    "    if top_k:\n",
    "        data = data[:top_k]\n",
    "\n",
    "    with plt.xkcd():\n",
    "        plt.barh([word for word, count in data], [count for word, count in data])\n",
    "        if xlim:\n",
    "            plt.xlim([0, 1])\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.xlabel(\"probability\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "for word in (\"вряд\", \"хочу\"):\n",
    "    plot_distribution(model.probs[(word,)], top_k=15, title=word + \" ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03927a46",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb756369",
   "metadata": {},
   "source": [
    "$$\n",
    "p(i)_T = \\frac{p(i) ^ \\frac{1}{T}}{\\displaystyle \\sum_j p(j) ^ \\frac{1}{T}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30d7efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_token(\n",
    "    lm: LanguageModel, prefix: List[str], temperature: float = 1.0\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    * получение распределения вероятностей для следующего слова при условии префикса\n",
    "    * сэмплирование из полученного распределения с температурой\n",
    "    \"\"\"\n",
    "\n",
    "    distribution: Dict[str, float] = lm.get_token_distribution(prefix)\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cda9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(\n",
    "    word=\"компьютер\",\n",
    "    temperature=widgets.FloatSlider(\n",
    "        value=1, min=0.01, max=3.0, step=0.2, description=\"Temperature:\"\n",
    "    ),\n",
    "    top_k=widgets.IntSlider(value=10, min=5, max=20, step=1, description=\"top_k:\"),\n",
    ")\n",
    "def plot_with_temperature(word: str, temperature: float, top_k: int):\n",
    "    distr = model.get_token_distribution(prefix=[word])\n",
    "    distr = {k: v ** (1.0 / temperature.real) for k, v in distr.items()}\n",
    "    norm = sum(distr.values())\n",
    "    distr = {k: v / norm for k, v in distr.items()}\n",
    "\n",
    "    title = f\"{word} (T = {round(temperature.real, 2):.2f})\"\n",
    "    plot_distribution(distr, top_k=top_k, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7764fa6",
   "metadata": {},
   "source": [
    "# Perplexity\n",
    "\n",
    "$$\n",
    "P(w_1, \\dots, w_N)^{-\\frac1N} = \\left( \\prod_t P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1})\\right)^{-\\frac1N} = \\frac{1}{\\sqrt[\\leftroot{-2}\\uproot{2}N]{\\displaystyle \\prod_t P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1})}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "e^{\\displaystyle \\log P(w_1, \\dots, w_N)^{-\\frac1N}} = e^{\\displaystyle -\\frac1N \\log P(w_1, \\dots, w_N)} = e^{\\displaystyle -\\frac1N \\log \\left( \\prod_t P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1}) \\right)} = e^{\\displaystyle -\\frac1N \\sum_t \\log P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8781587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(\n",
    "    lm: LanguageModel,\n",
    "    tokenized_texts: Iterable[Iterable[str]],\n",
    "    min_logprob: float = np.log(10**-50.0),\n",
    ") -> float:\n",
    "    logprobs_sum: float = 0.0\n",
    "    N: int = 0\n",
    "    for tokens in tokenized_texts:\n",
    "        prefix = [BOS] * (lm.n - 1)\n",
    "        padded_tokens = tokens + [EOS]\n",
    "        for token in padded_tokens:\n",
    "            logprob = np.log(lm.get_next_token_prob(prefix, token))\n",
    "            prefix = prefix[1:] + [token]\n",
    "            logprobs_sum += max(logprob, min_logprob)\n",
    "        N += len(padded_tokens)\n",
    "    return np.exp(-logprobs_sum / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8170cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity(model, val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aa2b32",
   "metadata": {},
   "source": [
    "# Laplace Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ee2def",
   "metadata": {},
   "source": [
    "Maximum Likelihood Estimation: $$\n",
    "    P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1}) \\approx \\frac{\\text{count} (w_{t - n + 1} \\dots, w_{t - 1}, w_t)}{\\displaystyle \\sum_w \\text{count} (w_{t - n + 1} \\dots w_{t - 1}, w)}=\\frac{\\text{count} (w_{t - n + 1} \\dots, w_{t - 1}, w_t)}{\\text{count} (w_{t - n + 1} \\dots w_{t - 1})}\n",
    "$$\n",
    "\n",
    "Laplace Smoothing: $$\n",
    "P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1}) \\approx \\frac{\\text{count} (w_{t - n + 1} \\dots, w_{t - 1}, w_t) + 1}{\\displaystyle \\sum_w \\left( \\text{count} (w_{t - n + 1} \\dots w_{t - 1}, w) + 1 \\right)} = \\frac{\\text{count} (w_{t - n + 1} \\dots, w_{t - 1}, w_t) + 1}{\\text{count} (w_{t - n + 1} \\dots w_{t - 1}) + \\lvert V \\rvert}\n",
    "$$\n",
    "\n",
    "$\\delta$-Smoothing: $$\n",
    "P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1}) \\approx \\frac{\\text{count} (w_{t - n + 1} \\dots, w_{t - 1}, w_t) + \\delta}{\\displaystyle \\sum_w \\left( \\text{count} (w_{t - n + 1} \\dots w_{t - 1}, w) + \\delta \\right)} = \\frac{\\text{count} (w_{t - n + 1} \\dots, w_{t - 1}, w_t) + \\delta}{\\text{count} (w_{t - n + 1} \\dots w_{t - 1}) + \\delta \\lvert V \\rvert}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f58f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplaceLanguageModel(LanguageModel):\n",
    "    def __init__(\n",
    "        self, tokenized_texts: Iterable[Iterable[str]], n: int, delta: float = 1.0\n",
    "    ):\n",
    "        self.n = n\n",
    "        ngram_counts = build_ngram_counts(tokenized_texts, n)\n",
    "\n",
    "        self.vocab = {\n",
    "            token for distribution in ngram_counts.values() for token in distribution\n",
    "        }\n",
    "\n",
    "        self.probs = defaultdict(Counter)\n",
    "\n",
    "        for prefix, distribution in ngram_counts.items():\n",
    "            norm: float = sum(distribution.values()) + delta * len(self.vocab)\n",
    "            self.probs[prefix] = {\n",
    "                token: (count + delta) / norm for token, count in distribution.items()\n",
    "            }\n",
    "\n",
    "    def get_token_distribution(self, prefix: List[str]) -> Dict[str, float]:\n",
    "        distribution: Dict[str, float] = super().get_token_distribution(prefix)\n",
    "        missing_prob_total: float = 1.0 - sum(distribution.values())\n",
    "        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(distribution))\n",
    "        return {token: distribution.get(token, missing_prob) for token in self.vocab}\n",
    "\n",
    "    def get_next_token_prob(self, prefix: List[str], next_token: str):\n",
    "        distribution: Dict[str, float] = super().get_token_distribution(prefix)\n",
    "        if next_token in distribution:\n",
    "            return distribution[next_token]\n",
    "        else:\n",
    "            missing_prob_total = 1.0 - sum(distribution.values())\n",
    "            return max(0, missing_prob_total) / max(\n",
    "                1, len(self.vocab) - len(distribution)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c703d22",
   "metadata": {},
   "source": [
    "перплексия снизилась, но остается большой, поскольку вероятностные распределения сильно искажаются сглаживанием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb4345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in (1, 2, 3):\n",
    "    laplace_model = LaplaceLanguageModel(train_set, n=n)\n",
    "    print(f\"{n}: {perplexity(laplace_model, val_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f409058",
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace_model = LaplaceLanguageModel(train_set, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cd9c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(model.get_token_distribution([\"машинное\"]), top_k=10, title=\"mle lm\")\n",
    "plot_distribution(\n",
    "    laplace_model.get_token_distribution([\"машинное\"]),\n",
    "    top_k=10,\n",
    "    title=\"laplace smoothing lm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5559b0ee",
   "metadata": {},
   "source": [
    "попробуем менее возмущающее сглаживание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14fd8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_model = LaplaceLanguageModel(train_set, n=2, delta=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a3c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity(delta_model, val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087c9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"мама мыла\".split()\n",
    "\n",
    "top_k = 5\n",
    "max_tokens = 10\n",
    "temperature = 1.1\n",
    "\n",
    "for _ in range(top_k):\n",
    "    generated = prefix[:]\n",
    "    for _ in range(max_tokens):\n",
    "        next_token = get_next_token(laplace_model, generated, temperature=temperature)\n",
    "        generated.append(next_token)\n",
    "        if next_token == EOS:\n",
    "            break\n",
    "    print(\" \".join(generated))\n",
    "    print(\"#\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65351bf",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a1c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.12.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e79bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/rugpt3small_based_on_gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ai-forever/rugpt3small_based_on_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14622765",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tensors_dict = tokenizer(\n",
    "    text=\"Сегодня состоялась лекция по machine learning\", return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "generated = model.generate(\n",
    "    **tokenized_tensors_dict, max_length=100, temperature=1.0, do_sample=True\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(generated.numpy()[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
