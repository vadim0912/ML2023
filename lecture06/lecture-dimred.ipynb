{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <center><img src=\"images/header1.png\" width=400></center> </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<h1><center>Основы машинного обучения</center></h1>\n",
    "<hr>\n",
    "<h2><center>Методы обучения без учителя: Методы понижения размерности</center></h2>\n",
    "<h3><center>Лектор: Ефимов Владислав</center></h3>\n",
    "<h3><center>Лекции готовили: Шестаков Андрей, Ефимов Владислав</center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T17:12:47.573130Z",
     "start_time": "2021-11-10T17:12:47.568172Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T17:12:47.584967Z",
     "start_time": "2021-11-10T17:12:47.576840Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "plt.style.use('seaborn-talk')\n",
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Для кириллицы на графиках\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "try:\n",
    "    from ipywidgets import interact, IntSlider, fixed, FloatSlider\n",
    "except ImportError:\n",
    "    print(u'Так надо')\n",
    "    \n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Методы понижения размерности признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Для чего можно понижать размерность признаков?\n",
    "\n",
    "* Cмотреть на 2-3 признака удобнее чем на 100\n",
    "* Потенциально может улучшить качество моделей\n",
    "* Удаление лишних коррелирующих признаков\n",
    "* Ускоряет обучение\n",
    "* Данные занимают меньше места\n",
    "* Меньше признаков - легче интерпретировать модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Способы понижения размерности\n",
    "\n",
    "Избавляться от размерности можно методами **отбора признаков (Feature Selection)** и методами **уменьшения размерности (Feature Reduction)**\n",
    "\n",
    "**Глобальная разница**\n",
    "* Feature Selection: не используем часть признаков\n",
    "* Feature Reduction: исходные признаки проходят через некоторое преобразование $f(\\cdot)$, и на выходе признаков становится меньше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principal Component Analysis\n",
    "## Метод Главных Компонент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Интерпретация\n",
    "\n",
    "* Интерпретация 1: находит такие ортогональные оси, вдоль которых дисперсия данных максимальна\n",
    "\n",
    "<center><img src='http://www.visiondummy.com/wp-content/uploads/2014/05/correlated_2d.png' width=400></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/pca.png' width=800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Интерпретация\n",
    "\n",
    "* Интерпретация 2: Найти такое подпространство меньшей размерности $L$ Такое что различие между точками и их проекциями минимальна\n",
    "\n",
    "<center><img src='images/plane_best_fit.png' width=800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/pca_example.png' width=800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## В двух словах:\n",
    "\n",
    "* Совершаем переход к новым осям, так что:\n",
    "    * Новые переменные являются линейной комбинацией старых переменных\n",
    "    * Новые оси ортогональны друг другу\n",
    "    * Дисперсия вдоль новых осей максимальная"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Что значит \"перейти к новым координатам\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Пусть у нас есть объект $x$ с тремя признаками: $x=(-0.343, -0.754, 0.241)$\n",
    "* Можно сказать, что он представлен в пространстве, натянутом на 3 стандартных базистых вектора:\n",
    "$$ e_1 = \\left( \\begin{array}{c}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{array} \\right) \\quad\n",
    "e_2 = \\left( \\begin{array}{c}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{array} \\right) \\quad\n",
    "e_3 = \\left( \\begin{array}{c}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{array} \\right) \\quad$$\n",
    "\n",
    "$$ x = -0.343 e_1 + -0.754 e_2 + 0.241 e_3 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Предположими мы хотим перейти к другому базису, например:\n",
    "\n",
    "$$ w_1 = \\left( \\begin{array}{c}\n",
    "-0.390 \\\\\n",
    "0.089 \\\\\n",
    "-0.916\n",
    "\\end{array} \\right) \\quad\n",
    "w_2 = \\left( \\begin{array}{c}\n",
    "-0.639 \\\\\n",
    "-0.742 \\\\\n",
    "0.200\n",
    "\\end{array} \\right) \\quad\n",
    "w_3 = \\left( \\begin{array}{c}\n",
    "-0.663 \\\\\n",
    "0.664 \\\\\n",
    "0.346\n",
    "\\end{array} \\right) \\quad$$\n",
    "* Как спроицировать наш объект $x$ из исходного базиса в данный?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Проецируем\n",
    "\n",
    "$$ w_1 = \\left( \\begin{array}{c}\n",
    "-0.390 \\\\\n",
    "0.089 \\\\\n",
    "-0.916\n",
    "\\end{array} \\right) \\quad\n",
    "w_2 = \\left( \\begin{array}{c}\n",
    "-0.639 \\\\\n",
    "-0.742 \\\\\n",
    "0.200\n",
    "\\end{array} \\right) \\quad\n",
    "w_3 = \\left( \\begin{array}{c}\n",
    "-0.663 \\\\\n",
    "0.664 \\\\\n",
    "0.346\n",
    "\\end{array} \\right) \\quad$$\n",
    "\n",
    "\n",
    "$$ z = W^\\top x = \\left( \\begin{array}{ccc}\n",
    "-0.390 & 0.089 & -0.916\\\\\n",
    "-0.639 & -0.742 & 0.200 \\\\\n",
    "-0.663 & 0.664 & 0.346\n",
    "\\end{array} \\right)\n",
    "\\left( \\begin{array}{c}\n",
    "-0.343 \\\\\n",
    "-0.754 \\\\\n",
    "0.241\n",
    "\\end{array} \\right) = \n",
    "\\left( \\begin{array}{c}\n",
    "-1.154 \\\\\n",
    "0.828 \\\\\n",
    "0.190\n",
    "\\end{array} \\right)$$\n",
    "\n",
    "То есть: $$ x = -1.154 w_1 + 0.828 w_2 + 0.190 w_3$$\n",
    "\n",
    "(Пример взят из [Mohammed J. Zaki, Ch7](https://www.amazon.com/Data-Mining-Analysis-Fundamental-Algorithms/dp/0521766338) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Проецировать научились, осталось только понять как же находить эти самые $w$?\n",
    "    * Новые оси ортогональны друг другу\n",
    "    * Дисперсия вдоль новых осей максимальна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Находим главные компоненты\n",
    "* Новые оси ортогональны друг другу:\n",
    "    $$\\langle w_{i},w_{j}\\rangle=\\begin{cases}\n",
    "1, & i=j\\\\\n",
    "0 & i\\ne j\n",
    "\\end{cases}$$\n",
    "\n",
    "* Дисперсия вдоль новых осей максимальна\n",
    "    * Предполагаем, что исходные признаки **центрированы**\n",
    "$$\n",
    "\\begin{align} \\sigma^2_w & = \\frac{1}{n-1}\\sum\\limits_{i=1}^n(z_i - \\mu_z)^2 \\\\\n",
    "& = \\frac{1}{n-1}\\sum\\limits_{i=1}^n(w^\\top x_i)^2 \\\\\n",
    "& = \\frac{1}{n-1}\\sum\\limits_{i=1}^n w^\\top( x_i x_i^\\top) w \\\\\n",
    "& = w^\\top \\left(\\frac{1}{n-1}\\sum\\limits_{i=1}^n x_i x_i^\\top \\right) w \\\\\n",
    "& = w^\\top C w \\rightarrow \\\\\n",
    "& = w^\\top \\frac{1}{n-1} X^\\top X w \\rightarrow \\max_w \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "* $C = \\frac{1}{n-1} X^\\top X$ - ковариационная матрица\n",
    "\n",
    "* [Interpreting Covariance Matrix](http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Находим главные компоненты\n",
    "То есть, чтобы найти первую главную компоненту, надо решить такую задачу:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "w_1^\\top C w_1  \\rightarrow \\max_{w_1} \\\\\n",
    "w_1^\\top w_1 = 1\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Решение для первой компоненты\n",
    "\n",
    "Изначально было так \n",
    "$$ w_1^\\top C w_1  \\rightarrow \\max_{w_1}$$\n",
    "\n",
    "* Строим лагранжиан\n",
    "$$ \\mathcal{L}(w_1, \\nu) = w_1^\\top C w_1 - \\nu (w_1^\\top w_1 - 1) \\rightarrow max_{w_1, \\nu}$$\n",
    "* Считем производную по $w_1$\n",
    "$$ \\frac{\\partial\\mathcal{L}}{\\partial w_1} = C w_1 - \\nu w_1 = 0 \\Leftrightarrow X^\\top X w_1 = \\nu w_1$$\n",
    "\n",
    "Подставим одно в другое:\n",
    "$$ w_1^\\top C w_1 = \\nu w_1^\\top w_1 = \\nu \\rightarrow \\max$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Задача о собственном векторе, собственном числе матрицы\n",
    "\n",
    "Оказывается, что у матрицы ковариций $C$:\n",
    "* Все собственные числа $\\lambda_i \\in \\mathbb{R}, \\lambda_i \\geq 0$ (упорядочим индексы по возрастанию так, что $\\lambda_1 \\geq \\lambda_2  \\geq \\dots \\geq \\lambda_d $)\n",
    "* Собственные вектора при $\\lambda_i \\neq \\lambda_j $ ортогональны: $v_i^\\top v_j = 0$\n",
    "* Для уникальных $\\lambda_i$ уникальны и $v_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Решение для первой компоненты\n",
    "\n",
    "Изначально было так \n",
    "$$ w_1^\\top C w_1  \\rightarrow \\max_{w_1}$$\n",
    "\n",
    "* Строим лагранжиан\n",
    "$$ \\mathcal{L}(w_1, \\nu) = w_1^\\top C w_1 - \\nu (w_1^\\top w_1 - 1) \\rightarrow max_{w_1, \\nu}$$\n",
    "* Считем производную по $w_1$\n",
    "$$ \\frac{\\partial\\mathcal{L}}{\\partial w_1} = C w_1 - \\nu w_1 = 0 \\Leftrightarrow С w_1 = \\nu w_1$$\n",
    "\n",
    "Подставим одно в другое:\n",
    "$$ w_1^\\top C w_1 = \\nu w_1^\\top w_1 = \\nu \\rightarrow \\max$$\n",
    "\n",
    "Что значит, что:\n",
    "* $\\nu$ - наибольшее собственное число матрицы $C$, а именно - $\\lambda_1$\n",
    "* $w_1$ - собственный вектор при $\\lambda_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Задача и решение для второй компоненты\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "w_2^\\top C w_2  \\rightarrow \\max_{w_2} \\\\\n",
    "w_2^\\top w_2 = 1 \\\\\n",
    "w_2^\\top w_1 = 0\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Аналогичными операциями мы прийдем к тому, что  $w_2$ - собственный вектор при втором по величине собственном числе матрицы $C$. \n",
    "\n",
    "То есть поиск главных компонент сводится к поиску собственных векторов и собственных чисел матрицы $C$!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Алгоритм\n",
    "\n",
    "1. Центрируем и _шкалируем_ исходные признаки\n",
    "\n",
    "    1.1. Вообще шкалирование для самого алгоритма не обязательно, как мы видели из выкладок выше. Применяя шкалирование мы изначально \"придаем\" данным форму единичной сферы.\n",
    "2. Считаем матрицу ковариаций $\\frac{1}{n-1}X^\\top X$\n",
    "3. Находим $k$ главных компонент и собственных чисел\n",
    "$$W = \n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "    \\mid & \\mid & & \\mid\\\\\n",
    "    w_{1} & w_{2} & \\ldots & w_{k} \\\\\n",
    "    \\mid & \\mid & & \\mid \n",
    "  \\end{array}\n",
    "\\right]\n",
    "$$\n",
    "4. Проецируем на них:\n",
    "$$ Z = XW $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Сколько компонент брать или зачем нужны $\\lambda$?\n",
    "\n",
    "* Из предыдущих слайдов мы помним, что \n",
    "$$w_1^\\top C w_1 = \\lambda_1$$\n",
    "То есть $\\lambda_1$ равна дисперсии точек, спроецированных на первую базовую компоненту\n",
    "* Величина\n",
    "$$\n",
    "\\frac{\\lambda_{i}}{\\sum_{d=1}^{D}\\lambda_{d}}\n",
    "$$\n",
    "Показывает долю объясненной дисперии компоненты $i$\n",
    "\n",
    "<center><img src='images/cumul_rat.png' width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Связь PCA и SVD\n",
    "\n",
    "1. Пусть все также X центрировано, $C = \\frac{1}{n-1} X^\\top X$ -- ковариационная матрица.\n",
    "2. Так как $C$ симметричная матрица, то она может быть представлена в виде $C = V L V^\\top$, где $V$ это матрица с собственными векторами $C$, а $L$ диагональня матрица с собственными значениями по убыванию.\n",
    "3. Сингулярное разложение позволяет нам представить произвольную матрицу в виде $X = U S V^\\top$, где $U, V$ унитарные матрицы, $S$ диагональная матрица с сингулярными значениями по убыванию.\n",
    "4. Подставим теперь разложение $X$, полученное через SVD, в определение $C$:\n",
    "$$\n",
    "C = \\frac{1}{n-1} V S U^\\top U S V^\\top = V \\frac{S^2}{n-1} V^\\top\n",
    "$$\n",
    "5. Теперь осталось только сравнить это выражение для $C$ с ее представлением через собственные вектора. Легко видеть, что $V$ это и есть собственные вектора матрицы ковариаций, а собственные ее значения связаны с сингулярными значениями как $\\lambda_i = s_i^2/(n-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MNIST, DIGITS\n",
    "<center><img src='images/digits.png' width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MNIST PCA\n",
    "\n",
    "<center><img src='./images/mnist_pca.jpg'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Резюме\n",
    "\n",
    "* PCA понижает размерность признакового пространства\n",
    "* Новые компоненты являются линейной комбинацией исходных признаков\n",
    "* Новые компоненты (не признаки) - ортогональны\n",
    "* Можно применять в моделях и для визуализации\n",
    "* [Tutorial 1](http://www.visiondummy.com/2014/05/feature-extraction-using-pca/)\n",
    "* [PCA SVD туториал](https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf)\n",
    "* [Относительно легкая и подробная статья про связь PCA, SVD и связь между ними](https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Многомерное шкалирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Идея\n",
    "\n",
    "* Перейти в пространство меньшей размерности так, чтобы расстояния между объектами в новом пространстве были подобны расстояниям в исходном пространстве.\n",
    "* Дано $X = [x_1,\\dots, x_n]\\in \\mathbb{R}^{N \\times D}$ и/или $\\delta_{ij}$ - мера близости между $(x_i,x_j)$\n",
    "* Надо найти $Y = [y_1,\\dots,y_n] \\in \\mathbb{R}^{N \\times d}$ такие, что $\\delta_{ij} \\approx d(y_i, y_j) = \\|y_i-y_j\\|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/mds.png' width =1200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Понятно, что точно воспроизвести расстояния получится не всегда"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/sphere_example.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Подходы\n",
    "* Классический (cMDS)\n",
    "* Метрический (metric MDS)\n",
    "* Неметрический (non-metric MDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# t-SNE\n",
    "## t-distributed stochastic neighbor embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* t-SNE - практически многомерное шкалирование\n",
    "* Вместо этого мы будем пытаться перенести \"окрестность\" точек из исходного пространства в пространоство меньшей размерности\n",
    "* Полученные расстояния скорее всего не будут соотносится с исходными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Схожесть между объектами в исходном пространстве $\\mathbb{R}^m$\n",
    "$$\n",
    "p(i, j) = \\frac{p(i | j) + p(j | i)}{2n}, \\quad p(j | i) = \\frac{\\exp(-\\|\\mathbf{x}_j-\\mathbf{x}_i\\|^2/{2 \\sigma_i^2})}{\\sum_{k \\neq i}\\exp(-\\|\\mathbf{x}_k-\\mathbf{x}_i\\|^2/{2 \\sigma_i^2})}\n",
    "$$\n",
    "$\\sigma_i$ неявно задается пользователем\n",
    "* Схожесть между объектами в целевом пространстве $\\mathbb{R}^k, k << m$\n",
    "$$\n",
    "q(i, j) = \\frac{g(|\\mathbf{y}_i - \\mathbf{y}_j|)}{\\sum_{k \\neq l} g(|\\mathbf{y}_i - \\mathbf{y}_j|)}\n",
    "$$ \n",
    "где $g(z) = \\frac{1}{1 + z^2}$ - распределение Коши (t-распределение Стьюдента с 1 степенью свободы)\n",
    "* Критерий\n",
    "$$\n",
    "J_{t-SNE}(y) = KL(P \\| Q) = \\sum_i \\sum_j p(i, j) \\log \\frac{p(i, j)}{q(i, j)} \\rightarrow \\min\\limits_{\\mathbf{y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Дивергенция Кульбака-Лейблера\n",
    "\n",
    "* Насколько распределение $P$ отличается от распределения $Q$?\n",
    "$$\n",
    "KL(P \\| Q) = \\sum_z P(z) \\log \\frac{P(z)}{Q(z)}\n",
    "$$\n",
    "\n",
    "<center><img src='images/kld.png' width=1200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Оптимизация\n",
    "\n",
    "* Оптимизируем $J_{t-SNE}(y)$ с помощью градиентного спуска\n",
    "\n",
    "$$\\frac{\\partial J_{t-SNE}}{\\partial y_i}=4 \\sum_j(p(i,j)−q(i,j))(y_i−y_j)g(|y_i−y_j|)$$\n",
    "\n",
    "* [Статья](http://jmlr.csail.mit.edu/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)\n",
    "* [Примеры](http://lvdmaaten.github.io/tsne/)\n",
    "* [Демо и советы](http://distill.pub/2016/misread-tsne/)\n",
    "    * t-SNE может быть нестабильным\n",
    "    * Размеры полученных сгустков могут ничего не значить\n",
    "    * Расстояния между кластерами могут ничего не значить\n",
    "    * Полностью шумовые данные могут выдать структуру"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='http://lvdmaaten.github.io/tsne/examples/mnist_tsne.jpg' width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/mainfold.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# UMAP\n",
    "## Uniform Manifold Approximation and Projection for Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Развитие подхода t-SNE.\n",
    "* Более быстрый за счет отсутствия нормализации для исходного и нового пространств.\n",
    "* Старается сохранить не только локальную, но и глобальную структуру в данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Схожесть между объектами в исходном пространстве $\\mathbb{R}^m$\n",
    "$$\n",
    "p_{ij} = p(i, j) = p(i | j) + p(j | i) - p(i | j)  p(j | i), \\quad p(i | j) = \\exp(-\\frac{d(x_i, x_j) - \\rho_i}{\\sigma_i})\n",
    "$$\n",
    "$d(x_i, x_j)$ -- расстояние между точками (не обязательно евклидово), $\\rho_i$ расстояние от $i$-го элемента до ближайшего соседа, $\\sigma_i$ ищутся исходя из уравнения ($k$ -- гиперпараметр, число соседей):\n",
    "$$\\sum_j p(i,j) = \\log_2 k$$\n",
    "* Схожесть между объектами в целевом пространстве $\\mathbb{R}^k, k < m$\n",
    "$$\n",
    "q_{ij} = q(i, j) = (1 + a(y_i - y_j)^{2b})^{-1}\n",
    "$$ \n",
    "где $a\\approx1.93$ и $b\\approx0.79$ гиперпараметры и их значения по-умолчанию.\n",
    "* Критерий -- fuzzy set cross entropy, кросс-энтропия для нечетких множеств\n",
    "$$\n",
    "CE(X, Y) = \\sum_i \\sum_j \\left[ p_{ij}(X)\\log\\frac{p_{ij}(X)}{q_{ij}(Y)} - (1 - p_{ij}(X))\\log\\frac{(1-p_{ij}(X))}{(1-q_{ij}(Y))} \\right]\n",
    "$$\n",
    "* Оптимизируемся с помощью стохастического градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* [Оригинальная статья](https://arxiv.org/abs/1802.03426)\n",
    "* [Статья с сравнением t-SNE и UMAP, а также более подробный разбор](https://towardsdatascience.com/how-exactly-umap-works-13e3040e1668)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/umap_vs_other.png' width=700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Литература\n",
    "* [Mohammed J. Zaki, Ch7](https://www.amazon.com/Data-Mining-Analysis-Fundamental-Algorithms/dp/0521766338)\n",
    "* [Мodern MDS](http://www.springer.com/la/book/9780387251509)\n",
    "* [Habrahabr](https://habrahabr.ru/post/267041/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Вопросы?\n",
    "\n",
    "### Пожалуйста, напишите отзыв о лекции"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/1f8c4751e12938961e423759861e6e5a"
  },
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "gist": {
   "data": {
    "description": "CloudMail/hse-da-course/raw/lecture-intro/lecture-intro-v01.ipynb",
    "public": false
   },
   "id": "1f8c4751e12938961e423759861e6e5a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "livereveal": {
   "theme": "serif",
   "transition": "concave"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "513px",
    "width": "253px"
   },
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "toc_position": {
   "height": "32px",
   "left": "9px",
   "right": "1379px",
   "top": "33px",
   "width": "212px"
  },
  "widgets": {
   "state": {
    "54e80d57f79b4bfc934a2b84cf5fe7ba": {
     "views": [
      {
       "cell_index": 47
      }
     ]
    },
    "5fb17a3592634a4fba98446dacd6db43": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "6f6f6ce7b81743308b92966f225862a8": {
     "views": [
      {
       "cell_index": 34
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
